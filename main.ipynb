{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from comet_ml import Experiment\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "from algorithms.server.server import Server\n",
    "from algorithms.trainmodel.models import *\n",
    "from utils.plot_utils import *\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def main(experiment, dataset, algorithm, model, batch_size, learning_rate, alpha, eta, L, rho, num_glob_iters,\n",
    "         local_epochs, optimizer, numedges, times, commet, gpu):\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "\n",
    "    for i in range(times):\n",
    "        print(\"---------------Running time:------------\",i)\n",
    "\n",
    "        # Generate model\n",
    "        if(model == \"mclr\"):\n",
    "            if(dataset == \"human_activity\"):\n",
    "                model = Mclr_Logistic(561,6).to(device), model\n",
    "            else:\n",
    "                model = Mclr_Logistic().to(device), model\n",
    "\n",
    "        if(model == \"linear_regression\"):\n",
    "            model = Linear_Regression(40,1).to(device), model\n",
    "\n",
    "        if model == \"logistic_regression\":\n",
    "            model = Logistic_Regression(300).to(device), model\n",
    "        \n",
    "        if model == \"MLP\" and dataset == \"a9a\":\n",
    "            model = DNN( input_dim = 123, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"human_activity\":\n",
    "            model = DNN( input_dim = 561, mid_dim = 561, output_dim = 6).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"w8a\":\n",
    "            model = DNN( input_dim = 300, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"Mnist\":\n",
    "            model = DNN().to(device), model\n",
    "\n",
    "        # select algorithm\n",
    "        if(commet):\n",
    "            experiment.set_name(dataset + \"_\" + algorithm + \"_\" + model[1] + \"_\" + str(batch_size) + \"b_\" + str(learning_rate) + \"lr_\" + str(alpha) + \"al_\" + str(eta) + \"eta_\" + str(L) + \"L_\" + str(rho) + \"p_\" +  str(num_glob_iters) + \"ge_\"+ str(local_epochs) + \"le_\"+ str(numedges) +\"u\")\n",
    "        server = Server(experiment, device, dataset, algorithm, model, batch_size, learning_rate, alpha, eta,  L, num_glob_iters, local_epochs, optimizer, numedges, i)\n",
    "        \n",
    "        server.train()\n",
    "        server.test()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sophia_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Sophia-1\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 0.01,\n",
    "    \"L\": 1e-5,\n",
    "    \"rho\": 20,\n",
    "    \"num_glob_iters\": 300,\n",
    "    \"local_epochs\": 20,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "DONE_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"DONE\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 500,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "Newton_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Newton\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 500,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"Newton\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/ahmed-khaled-saleh/sophia-supplement/b89fae61143444859237b4c71d00da6f\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     glob_acc [41]   : (0.06356438601732896, 0.9272482820436212)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [2675]     : (0.029319366440176964, 99.43136596679688)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_acc [41]  : (0.06619314641744549, 0.938018691588785)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [41] : (0.2140969430329278, 32.95207738503814)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : Mnist_Sophia-1_MLP_0b_0.0001lr_0.03al_0.1eta_1e-05L_20p_300ge_20le_30u\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (618.44 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/ahmed-khaled-saleh/sophia-supplement/1ba2ae14c4d74a4d85a22768c4d62eab\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Running time:------------ 0\n",
      "Number of edges / total edges: 30  /  32\n",
      "-------------Round number:  0  -------------\n",
      "Average Global Accuracy          :  0.09501045712578428\n",
      "Average Global Trainning Accuracy:  0.09228660436137072\n",
      "Average Global Trainning Loss    :  2.330636039376259\n",
      "-------------Round number:  1  -------------\n",
      "Average Global Accuracy          :  0.1200328652524649\n",
      "Average Global Trainning Accuracy:  0.12456074766355141\n",
      "Average Global Trainning Loss    :  2.2642328664660454\n",
      "-------------Round number:  2  -------------\n",
      "Average Global Accuracy          :  0.5910516880788766\n",
      "Average Global Trainning Accuracy:  0.5925981308411215\n",
      "Average Global Trainning Loss    :  2.0256411843001842\n",
      "-------------Round number:  3  -------------\n",
      "Average Global Accuracy          :  0.6094263519569764\n",
      "Average Global Trainning Accuracy:  0.6100436137071651\n",
      "Average Global Trainning Loss    :  1.9821207262575626\n",
      "-------------Round number:  4  -------------\n",
      "Average Global Accuracy          :  0.601434120107559\n",
      "Average Global Trainning Accuracy:  0.5986791277258567\n",
      "Average Global Trainning Loss    :  1.9605666808784008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error exporting current conda environment\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda package as an explicit file\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Round number:  5  -------------\n",
      "Average Global Accuracy          :  0.6119659396474455\n",
      "Average Global Trainning Accuracy:  0.6132336448598131\n",
      "Average Global Trainning Loss    :  1.9475578404963017\n",
      "-------------Round number:  6  -------------\n",
      "Average Global Accuracy          :  0.6154018524051389\n",
      "Average Global Trainning Accuracy:  0.6166978193146417\n",
      "Average Global Trainning Loss    :  1.9389541298151016\n",
      "-------------Round number:  7  -------------\n",
      "Average Global Accuracy          :  0.610920227069017\n",
      "Average Global Trainning Accuracy:  0.613582554517134\n",
      "Average Global Trainning Loss    :  1.9327624291181564\n",
      "-------------Round number:  8  -------------\n",
      "Average Global Accuracy          :  0.6118165521362414\n",
      "Average Global Trainning Accuracy:  0.6164485981308411\n",
      "Average Global Trainning Loss    :  1.928433321416378\n",
      "-------------Round number:  9  -------------\n",
      "Average Global Accuracy          :  0.6159994024499552\n",
      "Average Global Trainning Accuracy:  0.6181183800623054\n",
      "Average Global Trainning Loss    :  1.9254768751561642\n",
      "-------------Round number:  10  -------------\n",
      "Average Global Accuracy          :  0.6195847027188527\n",
      "Average Global Trainning Accuracy:  0.6202866043613707\n",
      "Average Global Trainning Loss    :  1.9231273271143436\n",
      "-------------Round number:  11  -------------\n",
      "Average Global Accuracy          :  0.6213026590976994\n",
      "Average Global Trainning Accuracy:  0.623177570093458\n",
      "Average Global Trainning Loss    :  1.9213807322084904\n",
      "-------------Round number:  12  -------------\n",
      "Average Global Accuracy          :  0.620481027786077\n",
      "Average Global Trainning Accuracy:  0.6229781931464174\n",
      "Average Global Trainning Loss    :  1.9198229387402534\n",
      "-------------Round number:  13  -------------\n",
      "Average Global Accuracy          :  0.6209291903196893\n",
      "Average Global Trainning Accuracy:  0.6218566978193146\n",
      "Average Global Trainning Loss    :  1.918781764805317\n",
      "-------------Round number:  14  -------------\n",
      "Average Global Accuracy          :  0.6194353152076486\n",
      "Average Global Trainning Accuracy:  0.6226542056074766\n",
      "Average Global Trainning Loss    :  1.9177319556474686\n",
      "-------------Round number:  15  -------------\n",
      "Average Global Accuracy          :  0.6184642963848223\n",
      "Average Global Trainning Accuracy:  0.621557632398754\n",
      "Average Global Trainning Loss    :  1.9167656302452087\n",
      "-------------Round number:  16  -------------\n",
      "Average Global Accuracy          :  0.6199581714968628\n",
      "Average Global Trainning Accuracy:  0.6227538940809969\n",
      "Average Global Trainning Loss    :  1.9159215353429317\n",
      "-------------Round number:  17  -------------\n",
      "Average Global Accuracy          :  0.6204063340304751\n",
      "Average Global Trainning Accuracy:  0.6236510903426792\n",
      "Average Global Trainning Loss    :  1.915138103067875\n",
      "-------------Round number:  18  -------------\n",
      "Average Global Accuracy          :  0.6194353152076486\n",
      "Average Global Trainning Accuracy:  0.6227788161993769\n",
      "Average Global Trainning Loss    :  1.9145383536815643\n",
      "-------------Round number:  19  -------------\n",
      "Average Global Accuracy          :  0.6191365401852406\n",
      "Average Global Trainning Accuracy:  0.623202492211838\n",
      "Average Global Trainning Loss    :  1.914011474698782\n",
      "-------------Round number:  20  -------------\n",
      "Average Global Accuracy          :  0.6198834777412608\n",
      "Average Global Trainning Accuracy:  0.6239501557632399\n",
      "Average Global Trainning Loss    :  1.913560513406992\n",
      "-------------Round number:  21  -------------\n",
      "Average Global Accuracy          :  0.6195847027188527\n",
      "Average Global Trainning Accuracy:  0.623601246105919\n",
      "Average Global Trainning Loss    :  1.9131442345678806\n",
      "-------------Round number:  22  -------------\n",
      "Average Global Accuracy          :  0.6204063340304751\n",
      "Average Global Trainning Accuracy:  0.6239501557632399\n",
      "Average Global Trainning Loss    :  1.9128645323216915\n",
      "-------------Round number:  23  -------------\n",
      "Average Global Accuracy          :  0.6207051090528832\n",
      "Average Global Trainning Accuracy:  0.6245233644859813\n",
      "Average Global Trainning Loss    :  1.912573914974928\n",
      "-------------Round number:  24  -------------\n",
      "Average Global Accuracy          :  0.6207798028084852\n",
      "Average Global Trainning Accuracy:  0.6250716510903427\n",
      "Average Global Trainning Loss    :  1.9123435579240322\n",
      "-------------Round number:  25  -------------\n",
      "Average Global Accuracy          :  0.6204063340304751\n",
      "Average Global Trainning Accuracy:  0.6243738317757009\n",
      "Average Global Trainning Loss    :  1.912074126303196\n",
      "-------------Round number:  26  -------------\n",
      "Average Global Accuracy          :  0.6206304152972811\n",
      "Average Global Trainning Accuracy:  0.6246978193146417\n",
      "Average Global Trainning Loss    :  1.911868266761303\n",
      "-------------Round number:  27  -------------\n",
      "Average Global Accuracy          :  0.6210785778308934\n",
      "Average Global Trainning Accuracy:  0.6251214953271028\n",
      "Average Global Trainning Loss    :  1.9116519205272198\n",
      "-------------Round number:  28  -------------\n",
      "Average Global Accuracy          :  0.6209291903196893\n",
      "Average Global Trainning Accuracy:  0.625619937694704\n",
      "Average Global Trainning Loss    :  1.9114285744726658\n",
      "-------------Round number:  29  -------------\n",
      "Average Global Accuracy          :  0.6212279653420975\n",
      "Average Global Trainning Accuracy:  0.6249221183800623\n",
      "Average Global Trainning Loss    :  1.9112439155578613\n",
      "-------------Round number:  30  -------------\n",
      "Average Global Accuracy          :  0.6221242904093218\n",
      "Average Global Trainning Accuracy:  0.6254704049844236\n",
      "Average Global Trainning Loss    :  1.9110876023769379\n",
      "-------------Round number:  31  -------------\n",
      "Average Global Accuracy          :  0.6224230654317299\n",
      "Average Global Trainning Accuracy:  0.6253208722741433\n",
      "Average Global Trainning Loss    :  1.9109892025589943\n",
      "-------------Round number:  32  -------------\n",
      "Average Global Accuracy          :  0.6223483716761279\n",
      "Average Global Trainning Accuracy:  0.6254704049844236\n",
      "Average Global Trainning Loss    :  1.9108105599880219\n",
      "-------------Round number:  33  -------------\n",
      "Average Global Accuracy          :  0.622647146698536\n",
      "Average Global Trainning Accuracy:  0.6254704049844236\n",
      "Average Global Trainning Loss    :  1.910668358206749\n",
      "-------------Round number:  34  -------------\n",
      "Average Global Accuracy          :  0.622497759187332\n",
      "Average Global Trainning Accuracy:  0.6262429906542056\n",
      "Average Global Trainning Loss    :  1.9105424731969833\n",
      "-------------Round number:  35  -------------\n",
      "Average Global Accuracy          :  0.6221242904093218\n",
      "Average Global Trainning Accuracy:  0.6249719626168224\n",
      "Average Global Trainning Loss    :  1.9104097075760365\n",
      "-------------Round number:  36  -------------\n",
      "Average Global Accuracy          :  0.6221242904093218\n",
      "Average Global Trainning Accuracy:  0.6258940809968847\n",
      "Average Global Trainning Loss    :  1.9102738872170448\n",
      "-------------Round number:  37  -------------\n",
      "Average Global Accuracy          :  0.6214520466089035\n",
      "Average Global Trainning Accuracy:  0.6250467289719626\n",
      "Average Global Trainning Loss    :  1.9101549834012985\n",
      "-------------Round number:  38  -------------\n",
      "Average Global Accuracy          :  0.6219002091425156\n",
      "Average Global Trainning Accuracy:  0.6245732087227415\n",
      "Average Global Trainning Loss    :  1.9100612364709377\n",
      "-------------Round number:  39  -------------\n",
      "Average Global Accuracy          :  0.6213773528533014\n",
      "Average Global Trainning Accuracy:  0.6248722741433022\n",
      "Average Global Trainning Loss    :  1.9099817350506783\n",
      "-------------Round number:  40  -------------\n",
      "Average Global Accuracy          :  0.6219002091425156\n",
      "Average Global Trainning Accuracy:  0.6260186915887851\n",
      "Average Global Trainning Loss    :  1.9098717235028744\n",
      "-------------Round number:  41  -------------\n",
      "Average Global Accuracy          :  0.6221989841649238\n",
      "Average Global Trainning Accuracy:  0.6253956386292835\n",
      "Average Global Trainning Loss    :  1.9097466543316841\n",
      "-------------Round number:  42  -------------\n",
      "Average Global Accuracy          :  0.6224230654317299\n",
      "Average Global Trainning Accuracy:  0.625196261682243\n",
      "Average Global Trainning Loss    :  1.9096497781574726\n",
      "-------------Round number:  43  -------------\n",
      "Average Global Accuracy          :  0.6224230654317299\n",
      "Average Global Trainning Accuracy:  0.6255451713395639\n",
      "Average Global Trainning Loss    :  1.9095713719725609\n",
      "-------------Round number:  44  -------------\n",
      "Average Global Accuracy          :  0.6236928592769645\n",
      "Average Global Trainning Accuracy:  0.625595015576324\n",
      "Average Global Trainning Loss    :  1.9094653874635696\n",
      "-------------Round number:  45  -------------\n",
      "Average Global Accuracy          :  0.6244397968329848\n",
      "Average Global Trainning Accuracy:  0.6256947040498443\n",
      "Average Global Trainning Loss    :  1.9093814045190811\n",
      "-------------Round number:  46  -------------\n",
      "Average Global Accuracy          :  0.6245891843441889\n",
      "Average Global Trainning Accuracy:  0.6266915887850467\n",
      "Average Global Trainning Loss    :  1.9093240462243557\n",
      "-------------Round number:  47  -------------\n",
      "Average Global Accuracy          :  0.6245891843441889\n",
      "Average Global Trainning Accuracy:  0.6266417445482866\n",
      "Average Global Trainning Loss    :  1.9092350825667381\n",
      "-------------Round number:  48  -------------\n",
      "Average Global Accuracy          :  0.6254855094114132\n",
      "Average Global Trainning Accuracy:  0.6271900311526479\n",
      "Average Global Trainning Loss    :  1.909129660576582\n",
      "-------------Round number:  49  -------------\n",
      "Average Global Accuracy          :  0.6251867343890051\n",
      "Average Global Trainning Accuracy:  0.6278878504672897\n",
      "Average Global Trainning Loss    :  1.9090730920433998\n",
      "-------------Round number:  50  -------------\n",
      "Average Global Accuracy          :  0.6251120406334031\n",
      "Average Global Trainning Accuracy:  0.6274641744548287\n",
      "Average Global Trainning Loss    :  1.9090080745518208\n",
      "-------------Round number:  51  -------------\n",
      "Average Global Accuracy          :  0.6253361219002092\n",
      "Average Global Trainning Accuracy:  0.6273146417445483\n",
      "Average Global Trainning Loss    :  1.9089356511831284\n",
      "-------------Round number:  52  -------------\n",
      "Average Global Accuracy          :  0.6263071407230355\n",
      "Average Global Trainning Accuracy:  0.6274890965732087\n",
      "Average Global Trainning Loss    :  1.9088866338133812\n",
      "-------------Round number:  53  -------------\n",
      "Average Global Accuracy          :  0.6260083657006275\n",
      "Average Global Trainning Accuracy:  0.6280124610591901\n",
      "Average Global Trainning Loss    :  1.908819917589426\n",
      "-------------Round number:  54  -------------\n",
      "Average Global Accuracy          :  0.6269046907678518\n",
      "Average Global Trainning Accuracy:  0.6280124610591901\n",
      "Average Global Trainning Loss    :  1.9087856821715832\n",
      "-------------Round number:  55  -------------\n",
      "Average Global Accuracy          :  0.6269793845234538\n",
      "Average Global Trainning Accuracy:  0.6277133956386293\n",
      "Average Global Trainning Loss    :  1.9087439216673374\n",
      "-------------Round number:  56  -------------\n",
      "Average Global Accuracy          :  0.6269793845234538\n",
      "Average Global Trainning Accuracy:  0.6271900311526479\n",
      "Average Global Trainning Loss    :  1.9086793325841427\n",
      "-------------Round number:  57  -------------\n",
      "Average Global Accuracy          :  0.6267553032566477\n",
      "Average Global Trainning Accuracy:  0.6268660436137071\n",
      "Average Global Trainning Loss    :  1.9086284078657627\n",
      "-------------Round number:  58  -------------\n",
      "Average Global Accuracy          :  0.6263818344786376\n",
      "Average Global Trainning Accuracy:  0.6270654205607477\n",
      "Average Global Trainning Loss    :  1.908552072942257\n",
      "-------------Round number:  59  -------------\n",
      "Average Global Accuracy          :  0.6263071407230355\n",
      "Average Global Trainning Accuracy:  0.6269906542056075\n",
      "Average Global Trainning Loss    :  1.908489491790533\n",
      "-------------Round number:  60  -------------\n",
      "Average Global Accuracy          :  0.6260083657006275\n",
      "Average Global Trainning Accuracy:  0.6265669781931464\n",
      "Average Global Trainning Loss    :  1.908430129289627\n",
      "-------------Round number:  61  -------------\n",
      "Average Global Accuracy          :  0.6258589781894234\n",
      "Average Global Trainning Accuracy:  0.6263676012461059\n",
      "Average Global Trainning Loss    :  1.9083595238626003\n",
      "-------------Round number:  62  -------------\n",
      "Average Global Accuracy          :  0.6254855094114132\n",
      "Average Global Trainning Accuracy:  0.6265171339563863\n",
      "Average Global Trainning Loss    :  1.90829598903656\n",
      "-------------Round number:  63  -------------\n",
      "Average Global Accuracy          :  0.6252614281446071\n",
      "Average Global Trainning Accuracy:  0.6265919003115265\n",
      "Average Global Trainning Loss    :  1.9082508645951748\n",
      "-------------Round number:  64  -------------\n",
      "Average Global Accuracy          :  0.6254855094114132\n",
      "Average Global Trainning Accuracy:  0.6262928348909658\n",
      "Average Global Trainning Loss    :  1.9082004465162754\n",
      "-------------Round number:  65  -------------\n",
      "Average Global Accuracy          :  0.6254108156558111\n",
      "Average Global Trainning Accuracy:  0.6260436137071651\n",
      "Average Global Trainning Loss    :  1.908112097531557\n",
      "-------------Round number:  66  -------------\n",
      "Average Global Accuracy          :  0.6256348969226173\n",
      "Average Global Trainning Accuracy:  0.6261183800623052\n",
      "Average Global Trainning Loss    :  1.9080561324954033\n",
      "-------------Round number:  67  -------------\n",
      "Average Global Accuracy          :  0.6260830594562294\n",
      "Average Global Trainning Accuracy:  0.6264922118380062\n",
      "Average Global Trainning Loss    :  1.9080113284289837\n",
      "-------------Round number:  68  -------------\n",
      "Average Global Accuracy          :  0.6262324469674335\n",
      "Average Global Trainning Accuracy:  0.6263177570093458\n",
      "Average Global Trainning Loss    :  1.9079829342663288\n",
      "-------------Round number:  69  -------------\n",
      "Average Global Accuracy          :  0.6260830594562294\n",
      "Average Global Trainning Accuracy:  0.626417445482866\n",
      "Average Global Trainning Loss    :  1.907959222793579\n",
      "-------------Round number:  70  -------------\n",
      "Average Global Accuracy          :  0.6264565282342396\n",
      "Average Global Trainning Accuracy:  0.626392523364486\n",
      "Average Global Trainning Loss    :  1.9079286195337772\n",
      "-------------Round number:  71  -------------\n",
      "Average Global Accuracy          :  0.6260083657006275\n",
      "Average Global Trainning Accuracy:  0.6258193146417446\n",
      "Average Global Trainning Loss    :  1.9078866131603718\n",
      "-------------Round number:  72  -------------\n",
      "Average Global Accuracy          :  0.6257095906782193\n",
      "Average Global Trainning Accuracy:  0.6258193146417446\n",
      "Average Global Trainning Loss    :  1.9078475646674633\n",
      "-------------Round number:  73  -------------\n",
      "Average Global Accuracy          :  0.6258589781894234\n",
      "Average Global Trainning Accuracy:  0.6258691588785047\n",
      "Average Global Trainning Loss    :  1.9078176282346249\n",
      "-------------Round number:  74  -------------\n",
      "Average Global Accuracy          :  0.6260083657006275\n",
      "Average Global Trainning Accuracy:  0.6257196261682243\n",
      "Average Global Trainning Loss    :  1.907771360129118\n",
      "-------------Round number:  75  -------------\n",
      "Average Global Accuracy          :  0.6259336719450254\n",
      "Average Global Trainning Accuracy:  0.6256697819314642\n",
      "Average Global Trainning Loss    :  1.907730370759964\n",
      "-------------Round number:  76  -------------\n",
      "Average Global Accuracy          :  0.6256348969226173\n",
      "Average Global Trainning Accuracy:  0.6253707165109035\n",
      "Average Global Trainning Loss    :  1.9076836220920086\n",
      "-------------Round number:  77  -------------\n",
      "Average Global Accuracy          :  0.6257095906782193\n",
      "Average Global Trainning Accuracy:  0.6249719626168224\n",
      "Average Global Trainning Loss    :  1.9076549299061298\n",
      "-------------Round number:  78  -------------\n",
      "Average Global Accuracy          :  0.6255602031670152\n",
      "Average Global Trainning Accuracy:  0.6251464174454828\n",
      "Average Global Trainning Loss    :  1.907637920230627\n",
      "-------------Round number:  79  -------------\n",
      "Average Global Accuracy          :  0.6255602031670152\n",
      "Average Global Trainning Accuracy:  0.6252211838006231\n",
      "Average Global Trainning Loss    :  1.9076106362044811\n",
      "-------------Round number:  80  -------------\n",
      "Average Global Accuracy          :  0.6258589781894234\n",
      "Average Global Trainning Accuracy:  0.625171339563863\n",
      "Average Global Trainning Loss    :  1.9075791873037815\n",
      "-------------Round number:  81  -------------\n",
      "Average Global Accuracy          :  0.6257095906782193\n",
      "Average Global Trainning Accuracy:  0.6252959501557632\n",
      "Average Global Trainning Loss    :  1.9075480587780476\n",
      "-------------Round number:  82  -------------\n",
      "Average Global Accuracy          :  0.6251867343890051\n",
      "Average Global Trainning Accuracy:  0.625171339563863\n",
      "Average Global Trainning Loss    :  1.9075181558728218\n",
      "-------------Round number:  83  -------------\n",
      "Average Global Accuracy          :  0.6254108156558111\n",
      "Average Global Trainning Accuracy:  0.624822429906542\n",
      "Average Global Trainning Loss    :  1.9074818976223469\n",
      "-------------Round number:  84  -------------\n",
      "Average Global Accuracy          :  0.6251867343890051\n",
      "Average Global Trainning Accuracy:  0.6249470404984424\n",
      "Average Global Trainning Loss    :  1.9074477292597294\n",
      "-------------Round number:  85  -------------\n",
      "Average Global Accuracy          :  0.6251120406334031\n",
      "Average Global Trainning Accuracy:  0.6247476635514019\n",
      "Average Global Trainning Loss    :  1.9074080772697926\n",
      "-------------Round number:  86  -------------\n",
      "Average Global Accuracy          :  0.624962653122199\n",
      "Average Global Trainning Accuracy:  0.6247725856697819\n",
      "Average Global Trainning Loss    :  1.9073774889111519\n",
      "-------------Round number:  87  -------------\n",
      "Average Global Accuracy          :  0.6253361219002092\n",
      "Average Global Trainning Accuracy:  0.6247227414330218\n",
      "Average Global Trainning Loss    :  1.9073534309864044\n",
      "-------------Round number:  88  -------------\n",
      "Average Global Accuracy          :  0.6251120406334031\n",
      "Average Global Trainning Accuracy:  0.6247476635514019\n",
      "Average Global Trainning Loss    :  1.9073236249387264\n",
      "-------------Round number:  89  -------------\n",
      "Average Global Accuracy          :  0.6251867343890051\n",
      "Average Global Trainning Accuracy:  0.6245981308411215\n",
      "Average Global Trainning Loss    :  1.9073025584220886\n",
      "-------------Round number:  90  -------------\n",
      "Average Global Accuracy          :  0.6256348969226173\n",
      "Average Global Trainning Accuracy:  0.6248722741433022\n",
      "Average Global Trainning Loss    :  1.9072908349335194\n",
      "-------------Round number:  91  -------------\n",
      "Average Global Accuracy          :  0.6254855094114132\n",
      "Average Global Trainning Accuracy:  0.624822429906542\n",
      "Average Global Trainning Loss    :  1.9072749130427837\n",
      "-------------Round number:  92  -------------\n",
      "Average Global Accuracy          :  0.6254108156558111\n",
      "Average Global Trainning Accuracy:  0.624797507788162\n",
      "Average Global Trainning Loss    :  1.907252922654152\n",
      "-------------Round number:  93  -------------\n",
      "Average Global Accuracy          :  0.6253361219002092\n",
      "Average Global Trainning Accuracy:  0.6242492211838007\n",
      "Average Global Trainning Loss    :  1.9072275534272194\n",
      "-------------Round number:  94  -------------\n",
      "Average Global Accuracy          :  0.625037346877801\n",
      "Average Global Trainning Accuracy:  0.6240996884735203\n",
      "Average Global Trainning Loss    :  1.9072113148868084\n",
      "-------------Round number:  95  -------------\n",
      "Average Global Accuracy          :  0.6251120406334031\n",
      "Average Global Trainning Accuracy:  0.6241246105919003\n",
      "Average Global Trainning Loss    :  1.9072013571858406\n",
      "-------------Round number:  96  -------------\n",
      "Average Global Accuracy          :  0.6254855094114132\n",
      "Average Global Trainning Accuracy:  0.6245732087227415\n",
      "Average Global Trainning Loss    :  1.90718212723732\n",
      "-------------Round number:  97  -------------\n",
      "Average Global Accuracy          :  0.6259336719450254\n",
      "Average Global Trainning Accuracy:  0.6246728971962617\n",
      "Average Global Trainning Loss    :  1.9071548394858837\n",
      "-------------Round number:  98  -------------\n",
      "Average Global Accuracy          :  0.6259336719450254\n",
      "Average Global Trainning Accuracy:  0.6242990654205608\n",
      "Average Global Trainning Loss    :  1.907130304723978\n",
      "-------------Round number:  99  -------------\n",
      "Average Global Accuracy          :  0.6257095906782193\n",
      "Average Global Trainning Accuracy:  0.6242741433021807\n",
      "Average Global Trainning Loss    :  1.9071237221360207\n",
      "-------------Round number:  100  -------------\n",
      "Average Global Accuracy          :  0.6254108156558111\n",
      "Average Global Trainning Accuracy:  0.6240747663551401\n",
      "Average Global Trainning Loss    :  1.907118421047926\n",
      "-------------Round number:  101  -------------\n",
      "Average Global Accuracy          :  0.6254108156558111\n",
      "Average Global Trainning Accuracy:  0.624\n",
      "Average Global Trainning Loss    :  1.9070999287068844\n",
      "-------------Round number:  102  -------------\n",
      "Average Global Accuracy          :  0.6256348969226173\n",
      "Average Global Trainning Accuracy:  0.6239003115264797\n",
      "Average Global Trainning Loss    :  1.9070808663964272\n",
      "-------------Round number:  103  -------------\n",
      "Average Global Accuracy          :  0.6257095906782193\n",
      "Average Global Trainning Accuracy:  0.6244236760124611\n",
      "Average Global Trainning Loss    :  1.9070688337087631\n",
      "-------------Round number:  104  -------------\n",
      "Average Global Accuracy          :  0.6252614281446071\n",
      "Average Global Trainning Accuracy:  0.6246479750778816\n",
      "Average Global Trainning Loss    :  1.907052967697382\n",
      "-------------Round number:  105  -------------\n",
      "Average Global Accuracy          :  0.6253361219002092\n",
      "Average Global Trainning Accuracy:  0.6246728971962617\n",
      "Average Global Trainning Loss    :  1.9070439226925373\n",
      "-------------Round number:  106  -------------\n",
      "Average Global Accuracy          :  0.6255602031670152\n",
      "Average Global Trainning Accuracy:  0.6249470404984424\n",
      "Average Global Trainning Loss    :  1.9070285856723785\n",
      "-------------Round number:  107  -------------\n",
      "Average Global Accuracy          :  0.6255602031670152\n",
      "Average Global Trainning Accuracy:  0.6247476635514019\n",
      "Average Global Trainning Loss    :  1.9070173986256123\n",
      "-------------Round number:  108  -------------\n",
      "Average Global Accuracy          :  0.6255602031670152\n",
      "Average Global Trainning Accuracy:  0.6248971962616823\n",
      "Average Global Trainning Loss    :  1.9070049151778221\n",
      "-------------Round number:  109  -------------\n",
      "Average Global Accuracy          :  0.6252614281446071\n",
      "Average Global Trainning Accuracy:  0.6247725856697819\n",
      "Average Global Trainning Loss    :  1.9069844409823418\n",
      "-------------Round number:  110  -------------\n",
      "Average Global Accuracy          :  0.6257095906782193\n",
      "Average Global Trainning Accuracy:  0.6248473520249221\n",
      "Average Global Trainning Loss    :  1.9069710448384285\n",
      "-------------Round number:  111  -------------\n",
      "Average Global Accuracy          :  0.6258589781894234\n",
      "Average Global Trainning Accuracy:  0.6249470404984424\n",
      "Average Global Trainning Loss    :  1.9069616869091988\n",
      "-------------Round number:  112  -------------\n",
      "Average Global Accuracy          :  0.6260830594562294\n",
      "Average Global Trainning Accuracy:  0.6250467289719626\n",
      "Average Global Trainning Loss    :  1.9069479703903198\n",
      "-------------Round number:  113  -------------\n",
      "Average Global Accuracy          :  0.6257842844338213\n",
      "Average Global Trainning Accuracy:  0.6250716510903427\n",
      "Average Global Trainning Loss    :  1.9069360569119453\n",
      "-------------Round number:  114  -------------\n",
      "Average Global Accuracy          :  0.6257842844338213\n",
      "Average Global Trainning Accuracy:  0.6251214953271028\n",
      "Average Global Trainning Loss    :  1.9069172963500023\n",
      "-------------Round number:  115  -------------\n",
      "Average Global Accuracy          :  0.6257095906782193\n",
      "Average Global Trainning Accuracy:  0.624822429906542\n",
      "Average Global Trainning Loss    :  1.906890358775854\n",
      "-------------Round number:  116  -------------\n",
      "Average Global Accuracy          :  0.6259336719450254\n",
      "Average Global Trainning Accuracy:  0.6254704049844236\n",
      "Average Global Trainning Loss    :  1.9068675488233566\n",
      "-------------Round number:  117  -------------\n",
      "Average Global Accuracy          :  0.6259336719450254\n",
      "Average Global Trainning Accuracy:  0.6254953271028038\n",
      "Average Global Trainning Loss    :  1.9068560488522053\n",
      "-------------Round number:  118  -------------\n",
      "Average Global Accuracy          :  0.6259336719450254\n",
      "Average Global Trainning Accuracy:  0.6250467289719626\n",
      "Average Global Trainning Loss    :  1.9068431966006756\n",
      "-------------Round number:  119  -------------\n",
      "Average Global Accuracy          :  0.6260083657006275\n",
      "Average Global Trainning Accuracy:  0.6253956386292835\n",
      "Average Global Trainning Loss    :  1.9068332873284817\n",
      "-------------Round number:  120  -------------\n",
      "Average Global Accuracy          :  0.6264565282342396\n",
      "Average Global Trainning Accuracy:  0.6252710280373832\n",
      "Average Global Trainning Loss    :  1.9068229719996452\n",
      "-------------Round number:  121  -------------\n",
      "Average Global Accuracy          :  0.6262324469674335\n",
      "Average Global Trainning Accuracy:  0.625595015576324\n",
      "Average Global Trainning Loss    :  1.9068080857396126\n",
      "-------------Round number:  122  -------------\n",
      "Average Global Accuracy          :  0.6263818344786376\n",
      "Average Global Trainning Accuracy:  0.625595015576324\n",
      "Average Global Trainning Loss    :  1.9067929536104202\n",
      "-------------Round number:  123  -------------\n",
      "Average Global Accuracy          :  0.6263071407230355\n",
      "Average Global Trainning Accuracy:  0.6256947040498443\n",
      "Average Global Trainning Loss    :  1.9067810326814651\n",
      "-------------Round number:  124  -------------\n",
      "Average Global Accuracy          :  0.6264565282342396\n",
      "Average Global Trainning Accuracy:  0.6256697819314642\n",
      "Average Global Trainning Loss    :  1.9067715853452682\n",
      "-------------Round number:  125  -------------\n",
      "Average Global Accuracy          :  0.6264565282342396\n",
      "Average Global Trainning Accuracy:  0.6256448598130842\n",
      "Average Global Trainning Loss    :  1.9067651480436325\n",
      "-------------Round number:  126  -------------\n",
      "Average Global Accuracy          :  0.6264565282342396\n",
      "Average Global Trainning Accuracy:  0.6257943925233644\n",
      "Average Global Trainning Loss    :  1.9067634157836437\n",
      "-------------Round number:  127  -------------\n",
      "Average Global Accuracy          :  0.6261577532118315\n",
      "Average Global Trainning Accuracy:  0.6257445482866043\n",
      "Average Global Trainning Loss    :  1.9067562259733677\n",
      "-------------Round number:  128  -------------\n",
      "Average Global Accuracy          :  0.6263071407230355\n",
      "Average Global Trainning Accuracy:  0.6261931464174455\n",
      "Average Global Trainning Loss    :  1.9067444540560246\n",
      "-------------Round number:  129  -------------\n",
      "Average Global Accuracy          :  0.6262324469674335\n",
      "Average Global Trainning Accuracy:  0.6263676012461059\n",
      "Average Global Trainning Loss    :  1.9067357890307903\n",
      "-------------Round number:  130  -------------\n",
      "Average Global Accuracy          :  0.6262324469674335\n",
      "Average Global Trainning Accuracy:  0.6265420560747663\n",
      "Average Global Trainning Loss    :  1.9067216254770756\n",
      "-------------Round number:  131  -------------\n",
      "Average Global Accuracy          :  0.6263818344786376\n",
      "Average Global Trainning Accuracy:  0.6267414330218068\n",
      "Average Global Trainning Loss    :  1.9067071452736855\n",
      "-------------Round number:  132  -------------\n",
      "Average Global Accuracy          :  0.6267553032566477\n",
      "Average Global Trainning Accuracy:  0.6269906542056075\n",
      "Average Global Trainning Loss    :  1.9066984206438065\n",
      "-------------Round number:  133  -------------\n",
      "Average Global Accuracy          :  0.6267553032566477\n",
      "Average Global Trainning Accuracy:  0.626791277258567\n",
      "Average Global Trainning Loss    :  1.9066837802529335\n",
      "-------------Round number:  134  -------------\n",
      "Average Global Accuracy          :  0.6268299970122497\n",
      "Average Global Trainning Accuracy:  0.626791277258567\n",
      "Average Global Trainning Loss    :  1.9066726751625538\n",
      "-------------Round number:  135  -------------\n",
      "Average Global Accuracy          :  0.6269046907678518\n",
      "Average Global Trainning Accuracy:  0.6265919003115265\n",
      "Average Global Trainning Loss    :  1.9066564813256264\n",
      "-------------Round number:  136  -------------\n",
      "Average Global Accuracy          :  0.6268299970122497\n",
      "Average Global Trainning Accuracy:  0.6267165109034268\n",
      "Average Global Trainning Loss    :  1.9066460579633713\n",
      "-------------Round number:  137  -------------\n",
      "Average Global Accuracy          :  0.6270540782790559\n",
      "Average Global Trainning Accuracy:  0.6268411214953271\n",
      "Average Global Trainning Loss    :  1.9066321067512035\n",
      "-------------Round number:  138  -------------\n",
      "Average Global Accuracy          :  0.6270540782790559\n",
      "Average Global Trainning Accuracy:  0.6269657320872274\n",
      "Average Global Trainning Loss    :  1.9066235609352589\n",
      "-------------Round number:  139  -------------\n",
      "Average Global Accuracy          :  0.6266059157454437\n",
      "Average Global Trainning Accuracy:  0.6271401869158878\n",
      "Average Global Trainning Loss    :  1.9066100530326366\n",
      "-------------Round number:  140  -------------\n",
      "Average Global Accuracy          :  0.6266059157454437\n",
      "Average Global Trainning Accuracy:  0.6269906542056075\n",
      "Average Global Trainning Loss    :  1.9065926298499107\n",
      "-------------Round number:  141  -------------\n",
      "Average Global Accuracy          :  0.6261577532118315\n",
      "Average Global Trainning Accuracy:  0.6270155763239875\n",
      "Average Global Trainning Loss    :  1.9065736085176468\n",
      "-------------Round number:  142  -------------\n",
      "Average Global Accuracy          :  0.6257842844338213\n",
      "Average Global Trainning Accuracy:  0.626766355140187\n",
      "Average Global Trainning Loss    :  1.9065513424575329\n",
      "-------------Round number:  143  -------------\n",
      "Average Global Accuracy          :  0.6259336719450254\n",
      "Average Global Trainning Accuracy:  0.6268411214953271\n",
      "Average Global Trainning Loss    :  1.9065269902348518\n",
      "-------------Round number:  144  -------------\n",
      "Average Global Accuracy          :  0.6260830594562294\n",
      "Average Global Trainning Accuracy:  0.626766355140187\n",
      "Average Global Trainning Loss    :  1.9065017104148865\n",
      "-------------Round number:  145  -------------\n",
      "Average Global Accuracy          :  0.6257842844338213\n",
      "Average Global Trainning Accuracy:  0.626766355140187\n",
      "Average Global Trainning Loss    :  1.906476765871048\n",
      "-------------Round number:  146  -------------\n",
      "Average Global Accuracy          :  0.6258589781894234\n",
      "Average Global Trainning Accuracy:  0.6270654205607477\n",
      "Average Global Trainning Loss    :  1.9064594320952892\n",
      "-------------Round number:  147  -------------\n",
      "Average Global Accuracy          :  0.6259336719450254\n",
      "Average Global Trainning Accuracy:  0.6269408099688474\n",
      "Average Global Trainning Loss    :  1.9064447283744812\n",
      "-------------Round number:  148  -------------\n",
      "Average Global Accuracy          :  0.6257095906782193\n",
      "Average Global Trainning Accuracy:  0.6267165109034268\n",
      "Average Global Trainning Loss    :  1.906426478177309\n",
      "-------------Round number:  149  -------------\n",
      "Average Global Accuracy          :  0.6251867343890051\n",
      "Average Global Trainning Accuracy:  0.626766355140187\n",
      "Average Global Trainning Loss    :  1.9064084403216839\n",
      "-------------Round number:  150  -------------\n",
      "Average Global Accuracy          :  0.6254855094114132\n",
      "Average Global Trainning Accuracy:  0.6266168224299066\n",
      "Average Global Trainning Loss    :  1.9063897766172886\n",
      "-------------Round number:  151  -------------\n",
      "Average Global Accuracy          :  0.6253361219002092\n",
      "Average Global Trainning Accuracy:  0.6265669781931464\n",
      "Average Global Trainning Loss    :  1.906366664916277\n",
      "-------------Round number:  152  -------------\n",
      "Average Global Accuracy          :  0.6252614281446071\n",
      "Average Global Trainning Accuracy:  0.6265420560747663\n",
      "Average Global Trainning Loss    :  1.9063562117516994\n",
      "-------------Round number:  153  -------------\n",
      "Average Global Accuracy          :  0.6251120406334031\n",
      "Average Global Trainning Accuracy:  0.6267165109034268\n",
      "Average Global Trainning Loss    :  1.9063462503254414\n",
      "-------------Round number:  154  -------------\n",
      "Average Global Accuracy          :  0.6251867343890051\n",
      "Average Global Trainning Accuracy:  0.6267414330218068\n",
      "Average Global Trainning Loss    :  1.9063307978212833\n",
      "-------------Round number:  155  -------------\n",
      "Average Global Accuracy          :  0.6251867343890051\n",
      "Average Global Trainning Accuracy:  0.6268411214953271\n",
      "Average Global Trainning Loss    :  1.906320657581091\n",
      "-------------Round number:  156  -------------\n",
      "Average Global Accuracy          :  0.6251120406334031\n",
      "Average Global Trainning Accuracy:  0.6270903426791278\n",
      "Average Global Trainning Loss    :  1.9063119776546955\n",
      "-------------Round number:  157  -------------\n",
      "Average Global Accuracy          :  0.6252614281446071\n",
      "Average Global Trainning Accuracy:  0.6269408099688474\n",
      "Average Global Trainning Loss    :  1.9062994457781315\n",
      "-------------Round number:  158  -------------\n",
      "Average Global Accuracy          :  0.6251867343890051\n",
      "Average Global Trainning Accuracy:  0.6269408099688474\n",
      "Average Global Trainning Loss    :  1.906284786760807\n",
      "-------------Round number:  159  -------------\n",
      "Average Global Accuracy          :  0.6256348969226173\n",
      "Average Global Trainning Accuracy:  0.6271401869158878\n",
      "Average Global Trainning Loss    :  1.906261246651411\n",
      "-------------Round number:  160  -------------\n",
      "Average Global Accuracy          :  0.6259336719450254\n",
      "Average Global Trainning Accuracy:  0.6269906542056075\n",
      "Average Global Trainning Loss    :  1.906241986900568\n",
      "-------------Round number:  161  -------------\n",
      "Average Global Accuracy          :  0.6256348969226173\n",
      "Average Global Trainning Accuracy:  0.6268909657320872\n",
      "Average Global Trainning Loss    :  1.9062296748161316\n",
      "-------------Round number:  162  -------------\n",
      "Average Global Accuracy          :  0.6254108156558111\n",
      "Average Global Trainning Accuracy:  0.6268411214953271\n",
      "Average Global Trainning Loss    :  1.9062193930149078\n",
      "-------------Round number:  163  -------------\n",
      "Average Global Accuracy          :  0.6253361219002092\n",
      "Average Global Trainning Accuracy:  0.626791277258567\n",
      "Average Global Trainning Loss    :  1.9062129072844982\n",
      "-------------Round number:  164  -------------\n",
      "Average Global Accuracy          :  0.624962653122199\n",
      "Average Global Trainning Accuracy:  0.6265669781931464\n",
      "Average Global Trainning Loss    :  1.9062015637755394\n",
      "-------------Round number:  165  -------------\n",
      "Average Global Accuracy          :  0.6251120406334031\n",
      "Average Global Trainning Accuracy:  0.6261183800623052\n",
      "Average Global Trainning Loss    :  1.9061866514384747\n",
      "-------------Round number:  166  -------------\n",
      "Average Global Accuracy          :  0.6252614281446071\n",
      "Average Global Trainning Accuracy:  0.6260186915887851\n",
      "Average Global Trainning Loss    :  1.9061719924211502\n",
      "-------------Round number:  167  -------------\n",
      "Average Global Accuracy          :  0.6254108156558111\n",
      "Average Global Trainning Accuracy:  0.6261433021806854\n",
      "Average Global Trainning Loss    :  1.9061405435204506\n",
      "-------------Round number:  168  -------------\n",
      "Average Global Accuracy          :  0.6253361219002092\n",
      "Average Global Trainning Accuracy:  0.6258940809968847\n",
      "Average Global Trainning Loss    :  1.9061272107064724\n",
      "-------------Round number:  169  -------------\n",
      "Average Global Accuracy          :  0.6252614281446071\n",
      "Average Global Trainning Accuracy:  0.625968847352025\n",
      "Average Global Trainning Loss    :  1.9061046540737152\n",
      "-------------Round number:  170  -------------\n",
      "Average Global Accuracy          :  0.6251867343890051\n",
      "Average Global Trainning Accuracy:  0.625595015576324\n",
      "Average Global Trainning Loss    :  1.9060887061059475\n",
      "-------------Round number:  171  -------------\n",
      "Average Global Accuracy          :  0.6253361219002092\n",
      "Average Global Trainning Accuracy:  0.6257943925233644\n",
      "Average Global Trainning Loss    :  1.9060770832002163\n",
      "-------------Round number:  172  -------------\n",
      "Average Global Accuracy          :  0.6254855094114132\n",
      "Average Global Trainning Accuracy:  0.6258193146417446\n",
      "Average Global Trainning Loss    :  1.9060674384236336\n",
      "-------------Round number:  173  -------------\n",
      "Average Global Accuracy          :  0.6253361219002092\n",
      "Average Global Trainning Accuracy:  0.6259439252336448\n",
      "Average Global Trainning Loss    :  1.906058106571436\n",
      "-------------Round number:  174  -------------\n",
      "Average Global Accuracy          :  0.6252614281446071\n",
      "Average Global Trainning Accuracy:  0.6260436137071651\n",
      "Average Global Trainning Loss    :  1.906048770993948\n",
      "-------------Round number:  175  -------------\n",
      "Average Global Accuracy          :  0.6255602031670152\n",
      "Average Global Trainning Accuracy:  0.625993769470405\n",
      "Average Global Trainning Loss    :  1.9060346148908138\n",
      "-------------Round number:  176  -------------\n",
      "Average Global Accuracy          :  0.6256348969226173\n",
      "Average Global Trainning Accuracy:  0.6262429906542056\n",
      "Average Global Trainning Loss    :  1.9060197435319424\n",
      "-------------Round number:  177  -------------\n",
      "Average Global Accuracy          :  0.6257842844338213\n",
      "Average Global Trainning Accuracy:  0.6265919003115265\n",
      "Average Global Trainning Loss    :  1.9060032963752747\n",
      "-------------Round number:  178  -------------\n",
      "Average Global Accuracy          :  0.6257842844338213\n",
      "Average Global Trainning Accuracy:  0.6263426791277259\n",
      "Average Global Trainning Loss    :  1.9059889279305935\n",
      "-------------Round number:  179  -------------\n",
      "Average Global Accuracy          :  0.6260083657006275\n",
      "Average Global Trainning Accuracy:  0.6263177570093458\n",
      "Average Global Trainning Loss    :  1.9059799276292324\n",
      "-------------Round number:  180  -------------\n",
      "Average Global Accuracy          :  0.6258589781894234\n",
      "Average Global Trainning Accuracy:  0.6263177570093458\n",
      "Average Global Trainning Loss    :  1.905967816710472\n",
      "-------------Round number:  181  -------------\n",
      "Average Global Accuracy          :  0.6257842844338213\n",
      "Average Global Trainning Accuracy:  0.6262679127725856\n",
      "Average Global Trainning Loss    :  1.9059578590095043\n",
      "-------------Round number:  182  -------------\n",
      "Average Global Accuracy          :  0.6257095906782193\n",
      "Average Global Trainning Accuracy:  0.6262928348909658\n",
      "Average Global Trainning Loss    :  1.9059442020952702\n",
      "-------------Round number:  183  -------------\n",
      "Average Global Accuracy          :  0.6260083657006275\n",
      "Average Global Trainning Accuracy:  0.6267165109034268\n",
      "Average Global Trainning Loss    :  1.905927699059248\n",
      "-------------Round number:  184  -------------\n",
      "Average Global Accuracy          :  0.6262324469674335\n",
      "Average Global Trainning Accuracy:  0.6265919003115265\n",
      "Average Global Trainning Loss    :  1.9059096947312355\n",
      "-------------Round number:  185  -------------\n",
      "Average Global Accuracy          :  0.6262324469674335\n",
      "Average Global Trainning Accuracy:  0.6268411214953271\n",
      "Average Global Trainning Loss    :  1.9058977663516998\n",
      "-------------Round number:  186  -------------\n",
      "Average Global Accuracy          :  0.6263071407230355\n",
      "Average Global Trainning Accuracy:  0.6269408099688474\n",
      "Average Global Trainning Loss    :  1.9058808200061321\n",
      "-------------Round number:  187  -------------\n",
      "Average Global Accuracy          :  0.6263071407230355\n",
      "Average Global Trainning Accuracy:  0.6269158878504673\n",
      "Average Global Trainning Loss    :  1.9058622755110264\n",
      "-------------Round number:  188  -------------\n",
      "Average Global Accuracy          :  0.6261577532118315\n",
      "Average Global Trainning Accuracy:  0.6270654205607477\n",
      "Average Global Trainning Loss    :  1.9058571830391884\n",
      "-------------Round number:  189  -------------\n",
      "Average Global Accuracy          :  0.6261577532118315\n",
      "Average Global Trainning Accuracy:  0.6270903426791278\n",
      "Average Global Trainning Loss    :  1.905836921185255\n",
      "-------------Round number:  190  -------------\n",
      "Average Global Accuracy          :  0.6264565282342396\n",
      "Average Global Trainning Accuracy:  0.6273395638629283\n",
      "Average Global Trainning Loss    :  1.905815452337265\n",
      "-------------Round number:  191  -------------\n",
      "Average Global Accuracy          :  0.6263071407230355\n",
      "Average Global Trainning Accuracy:  0.6272398753894081\n",
      "Average Global Trainning Loss    :  1.9057842940092087\n",
      "-------------Round number:  192  -------------\n",
      "Average Global Accuracy          :  0.6260083657006275\n",
      "Average Global Trainning Accuracy:  0.6273644859813085\n",
      "Average Global Trainning Loss    :  1.9057652987539768\n",
      "-------------Round number:  193  -------------\n",
      "Average Global Accuracy          :  0.6259336719450254\n",
      "Average Global Trainning Accuracy:  0.6271651090342679\n",
      "Average Global Trainning Loss    :  1.9057459570467472\n",
      "-------------Round number:  194  -------------\n",
      "Average Global Accuracy          :  0.6260083657006275\n",
      "Average Global Trainning Accuracy:  0.6271401869158878\n",
      "Average Global Trainning Loss    :  1.9057374186813831\n",
      "-------------Round number:  195  -------------\n",
      "Average Global Accuracy          :  0.6260083657006275\n",
      "Average Global Trainning Accuracy:  0.6270903426791278\n",
      "Average Global Trainning Loss    :  1.905715636909008\n",
      "-------------Round number:  196  -------------\n",
      "Average Global Accuracy          :  0.6261577532118315\n",
      "Average Global Trainning Accuracy:  0.6271651090342679\n",
      "Average Global Trainning Loss    :  1.9056973978877068\n",
      "-------------Round number:  197  -------------\n",
      "Average Global Accuracy          :  0.6265312219898417\n",
      "Average Global Trainning Accuracy:  0.6273894080996885\n",
      "Average Global Trainning Loss    :  1.90568046271801\n",
      "-------------Round number:  198  -------------\n",
      "Average Global Accuracy          :  0.6261577532118315\n",
      "Average Global Trainning Accuracy:  0.6273146417445483\n",
      "Average Global Trainning Loss    :  1.905668418854475\n",
      "-------------Round number:  199  -------------\n",
      "Average Global Accuracy          :  0.6262324469674335\n",
      "Average Global Trainning Accuracy:  0.6275638629283489\n",
      "Average Global Trainning Loss    :  1.9056471288204193\n",
      "-------------Round number:  200  -------------\n",
      "Average Global Accuracy          :  0.6259336719450254\n",
      "Average Global Trainning Accuracy:  0.6275638629283489\n",
      "Average Global Trainning Loss    :  1.9056311175227165\n",
      "-------------Round number:  201  -------------\n",
      "Average Global Accuracy          :  0.6263818344786376\n",
      "Average Global Trainning Accuracy:  0.6275389408099689\n",
      "Average Global Trainning Loss    :  1.9056186489760876\n",
      "-------------Round number:  202  -------------\n",
      "Average Global Accuracy          :  0.6263818344786376\n",
      "Average Global Trainning Accuracy:  0.6276386292834891\n",
      "Average Global Trainning Loss    :  1.905602179467678\n",
      "-------------Round number:  203  -------------\n",
      "Average Global Accuracy          :  0.6266059157454437\n",
      "Average Global Trainning Accuracy:  0.6278130841121495\n",
      "Average Global Trainning Loss    :  1.905590444803238\n",
      "-------------Round number:  204  -------------\n",
      "Average Global Accuracy          :  0.6263071407230355\n",
      "Average Global Trainning Accuracy:  0.6278878504672897\n",
      "Average Global Trainning Loss    :  1.905577413737774\n",
      "-------------Round number:  205  -------------\n",
      "Average Global Accuracy          :  0.6263071407230355\n",
      "Average Global Trainning Accuracy:  0.6278130841121495\n",
      "Average Global Trainning Loss    :  1.9055670090019703\n",
      "-------------Round number:  206  -------------\n",
      "Average Global Accuracy          :  0.6263818344786376\n",
      "Average Global Trainning Accuracy:  0.6278380062305295\n",
      "Average Global Trainning Loss    :  1.9055539146065712\n",
      "-------------Round number:  207  -------------\n",
      "Average Global Accuracy          :  0.6264565282342396\n",
      "Average Global Trainning Accuracy:  0.6279626168224299\n",
      "Average Global Trainning Loss    :  1.905541229993105\n",
      "-------------Round number:  208  -------------\n",
      "Average Global Accuracy          :  0.6262324469674335\n",
      "Average Global Trainning Accuracy:  0.6278130841121495\n",
      "Average Global Trainning Loss    :  1.9055172018706799\n",
      "-------------Round number:  209  -------------\n",
      "Average Global Accuracy          :  0.6263818344786376\n",
      "Average Global Trainning Accuracy:  0.6277632398753894\n",
      "Average Global Trainning Loss    :  1.9055014662444592\n",
      "-------------Round number:  210  -------------\n",
      "Average Global Accuracy          :  0.6267553032566477\n",
      "Average Global Trainning Accuracy:  0.6278878504672897\n",
      "Average Global Trainning Loss    :  1.9054827354848385\n",
      "-------------Round number:  211  -------------\n",
      "Average Global Accuracy          :  0.6263071407230355\n",
      "Average Global Trainning Accuracy:  0.6275389408099689\n",
      "Average Global Trainning Loss    :  1.9054471924901009\n",
      "-------------Round number:  212  -------------\n",
      "Average Global Accuracy          :  0.6263818344786376\n",
      "Average Global Trainning Accuracy:  0.6277881619937695\n",
      "Average Global Trainning Loss    :  1.905443575233221\n",
      "-------------Round number:  213  -------------\n",
      "Average Global Accuracy          :  0.6266059157454437\n",
      "Average Global Trainning Accuracy:  0.6278380062305295\n",
      "Average Global Trainning Loss    :  1.9054372906684875\n",
      "-------------Round number:  214  -------------\n",
      "Average Global Accuracy          :  0.6266059157454437\n",
      "Average Global Trainning Accuracy:  0.6277881619937695\n",
      "Average Global Trainning Loss    :  1.9053858071565628\n",
      "-------------Round number:  215  -------------\n",
      "Average Global Accuracy          :  0.6266059157454437\n",
      "Average Global Trainning Accuracy:  0.6276386292834891\n",
      "Average Global Trainning Loss    :  1.9053798764944077\n",
      "-------------Round number:  216  -------------\n",
      "Average Global Accuracy          :  0.6263071407230355\n",
      "Average Global Trainning Accuracy:  0.6276386292834891\n",
      "Average Global Trainning Loss    :  1.9053508751094341\n",
      "-------------Round number:  217  -------------\n",
      "Average Global Accuracy          :  0.6261577532118315\n",
      "Average Global Trainning Accuracy:  0.6276386292834891\n",
      "Average Global Trainning Loss    :  1.9053382314741611\n",
      "-------------Round number:  218  -------------\n",
      "Average Global Accuracy          :  0.6262324469674335\n",
      "Average Global Trainning Accuracy:  0.6277632398753894\n",
      "Average Global Trainning Loss    :  1.9053355269134045\n",
      "-------------Round number:  219  -------------\n",
      "Average Global Accuracy          :  0.6262324469674335\n",
      "Average Global Trainning Accuracy:  0.6279626168224299\n",
      "Average Global Trainning Loss    :  1.9053253456950188\n",
      "-------------Round number:  220  -------------\n",
      "Average Global Accuracy          :  0.6260830594562294\n",
      "Average Global Trainning Accuracy:  0.6278380062305295\n",
      "Average Global Trainning Loss    :  1.9053198657929897\n",
      "-------------Round number:  221  -------------\n",
      "Average Global Accuracy          :  0.6263818344786376\n",
      "Average Global Trainning Accuracy:  0.6279376947040498\n",
      "Average Global Trainning Loss    :  1.9053100049495697\n",
      "-------------Round number:  222  -------------\n",
      "Average Global Accuracy          :  0.6261577532118315\n",
      "Average Global Trainning Accuracy:  0.6280373831775701\n",
      "Average Global Trainning Loss    :  1.905297614634037\n",
      "-------------Round number:  223  -------------\n",
      "Average Global Accuracy          :  0.6262324469674335\n",
      "Average Global Trainning Accuracy:  0.6279376947040498\n",
      "Average Global Trainning Loss    :  1.9052912890911102\n",
      "-------------Round number:  224  -------------\n",
      "Average Global Accuracy          :  0.6263071407230355\n",
      "Average Global Trainning Accuracy:  0.6279875389408099\n",
      "Average Global Trainning Loss    :  1.9052881449460983\n",
      "-------------Round number:  225  -------------\n",
      "Average Global Accuracy          :  0.6263818344786376\n",
      "Average Global Trainning Accuracy:  0.6281370716510903\n",
      "Average Global Trainning Loss    :  1.9052833281457424\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"lhdQnruUATiAZPyU7Qp2zFiVX\",\n",
    "        project_name=\"sophia-supplement\",\n",
    "        workspace=\"ahmed-khaled-saleh\",\n",
    "    )\n",
    "main(experiment, **sophia_params)\n",
    "experiment.end()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"lhdQnruUATiAZPyU7Qp2zFiVX\",\n",
    "        project_name=\"sophia-supplement\",\n",
    "        workspace=\"ahmed-khaled-saleh\",\n",
    ")\n",
    "\n",
    "main(experiment, **DONE_params)\n",
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
