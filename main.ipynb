{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from comet_ml import Experiment\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "from algorithms.server.server import Server\n",
    "from algorithms.trainmodel.models import *\n",
    "from utils.plot_utils import *\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def main(trial):\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(\"0\") if torch.cuda.is_available() and \"0\" != -1 else \"cpu\")\n",
    "    algorithm = 'Sophia'\n",
    "    alpha = 0.03\n",
    "    eta = 0.1\n",
    "    L = 0.001\n",
    "    num_glob_iters = 100\n",
    "    local_epochs = 1, \n",
    "    optimizer = 'SGD'\n",
    "    numedges = 32,\n",
    "    tau = 10\n",
    "    model = \"MLP\"\n",
    "    rho = 0.01\n",
    "\n",
    "    in_dim = {\n",
    "        \"a9a\": 123,\n",
    "        \"Mnist\": 784,\n",
    "        \"w8a\": 300,\n",
    "        \"human_activity\": 561,\n",
    "        \"phishing\": 68,\n",
    "        \"linear_regression\": 40,\n",
    "\n",
    "    }\n",
    "\n",
    "    out_dim = {\n",
    "        \"a9a\": 2,\n",
    "        \"Mnist\": 10,\n",
    "        \"w8a\": 2,\n",
    "        \"human_activity\": 6,\n",
    "        \"phishing\": 2,\n",
    "    }\n",
    "    dataset = \"Mnist\"\n",
    "    for i in range(1):\n",
    "        print(\"---------------Running time:------------\",i)\n",
    "\n",
    "        # Generate model\n",
    "        if(model == \"mclr\"):\n",
    "            model = Mclr_CrossEntropy(input_dim = in_dim[dataset], output_dim = out_dim[dataset]).to(device), model\n",
    "\n",
    "        if(model == \"linear_regression\"):\n",
    "            model = Linear_Regression(input_dim = in_dim[dataset]).to(device), model\n",
    "\n",
    "        if model == \"logistic_regression\":\n",
    "            model = Logistic_Regression(input_dim = in_dim[dataset]).to(device), model\n",
    "        \n",
    "        if model == \"MLP\":\n",
    "            model = DNN(input_dim = in_dim[dataset], output_dim = out_dim[dataset]).to(device), model\n",
    "        \n",
    "        if model == 'CNN':\n",
    "            model = Net().to(device), model\n",
    "        commet = True\n",
    "\n",
    "        experiment = Experiment(\n",
    "            api_key=\"lhdQnruUATiAZPyU7Qp2zFiVX\",\n",
    "            project_name=\"automated-hyperparams\",\n",
    "            workspace=\"ahmed-khaled-saleh\",\n",
    "            )\n",
    "\n",
    "        \n",
    "        \n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [0, 16, 32, 64])\n",
    "        tau = trial.suggest_int('tau', 1, 10)\n",
    "        L = trial.suggest_loguniform('L', 1e-5, 1e-1)\n",
    "        local_epochs = trial.suggest_int('local_epochs', 1, 10)\n",
    "\n",
    "        if(commet): \n",
    "            experiment.set_name(dataset + \"_\" + algorithm + \"_\" + model[1] + \"_\" + str(batch_size) + \"b_\" + str(learning_rate) + \"lr_\" + str(alpha) + \"al_\" + str(eta) + \"eta_\" + str(L) + \"L_\" + str(rho) + \"p_\" +  str(num_glob_iters) + \"ge_\"+ str(local_epochs) + \"le_\"+ str(numedges) +\"u\" + str(tau) + \"tau\")\n",
    "        \n",
    "\n",
    "\n",
    "        server = Server(experiment, device, dataset, algorithm, model, batch_size, learning_rate, alpha, eta,  L, num_glob_iters, local_epochs, optimizer, numedges, i, tau)\n",
    "        \n",
    "        server.train()\n",
    "        server.test()\n",
    "        metrics  = server.evaluate()\n",
    "        loss, accuracy = metrics[1], metrics[2]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-12 16:02:58,777] A new study created in memory with name: no-name-e784569d-a3a0-49b7-ad44-8ba6efa6ae22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Running time:------------ 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/ahmed-khaled-saleh/automated-hyperparams/c12fcad8519c4de38bf5c5a50f79afb8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Not all initial data has been logged for experiment c12fcad8519c4de38bf5c5a50f79afb8, call Experiment.end() to ensure that all data to have been logged\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/ahmed-khaled-saleh/automated-hyperparams/bc3755af18af4290ae90600495c0ea06\n",
      "\n",
      "C:\\Users\\aelbakar23\\AppData\\Local\\Temp\\ipykernel_20528\\747080690.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "C:\\Users\\aelbakar23\\AppData\\Local\\Temp\\ipykernel_20528\\747080690.py:78: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  L = trial.suggest_loguniform('L', 1e-5, 1e-1)\n",
      "[W 2023-10-12 16:03:28,411] Trial 0 failed with parameters: {'learning_rate': 1.8679513496136032e-05, 'batch_size': 64, 'tau': 2, 'L': 0.08570987354769614, 'local_epochs': 9} because of the following error: NameError(\"name 'rho' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aelbakar23\\Anaconda3\\envs\\adev\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\aelbakar23\\AppData\\Local\\Temp\\ipykernel_20528\\747080690.py\", line 82, in main\n",
      "    experiment.set_name(dataset + \"_\" + algorithm + \"_\" + model[1] + \"_\" + str(batch_size) + \"b_\" + str(learning_rate) + \"lr_\" + str(alpha) + \"al_\" + str(eta) + \"eta_\" + str(L) + \"L_\" + str(rho) + \"p_\" +  str(num_glob_iters) + \"ge_\"+ str(local_epochs) + \"le_\"+ str(numedges) +\"u\" + str(tau) + \"tau\")\n",
      "NameError: name 'rho' is not defined\n",
      "[W 2023-10-12 16:03:28,417] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rho' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mk:\\optimization\\Fed-Sophia-pytorch\\main.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/k%3A/optimization/Fed-Sophia-pytorch/main.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/k%3A/optimization/Fed-Sophia-pytorch/main.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Start the optimization process\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/k%3A/optimization/Fed-Sophia-pytorch/main.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(main, n_trials\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\aelbakar23\\Anaconda3\\envs\\adev\\lib\\site-packages\\optuna\\study\\study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     _optimize(\n\u001b[0;32m    443\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    444\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    445\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    446\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    447\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    448\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    449\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    450\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    451\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    452\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\aelbakar23\\Anaconda3\\envs\\adev\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\aelbakar23\\Anaconda3\\envs\\adev\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\aelbakar23\\Anaconda3\\envs\\adev\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\aelbakar23\\Anaconda3\\envs\\adev\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mk:\\optimization\\Fed-Sophia-pytorch\\main.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/optimization/Fed-Sophia-pytorch/main.ipynb#W4sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m local_epochs \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39msuggest_int(\u001b[39m'\u001b[39m\u001b[39mlocal_epochs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/optimization/Fed-Sophia-pytorch/main.ipynb#W4sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mif\u001b[39;00m(commet): \n\u001b[1;32m---> <a href='vscode-notebook-cell:/k%3A/optimization/Fed-Sophia-pytorch/main.ipynb#W4sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     experiment\u001b[39m.\u001b[39mset_name(dataset \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m algorithm \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m model[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(batch_size) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(learning_rate) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlr_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(alpha) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mal_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(eta) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39meta_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(L) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mL_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(rho) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mp_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m  \u001b[39mstr\u001b[39m(num_glob_iters) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mge_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(local_epochs) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mle_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(numedges) \u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(tau) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtau\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/optimization/Fed-Sophia-pytorch/main.ipynb#W4sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m server \u001b[39m=\u001b[39m Server(experiment, device, dataset, algorithm, model, batch_size, learning_rate, alpha, eta,  L, num_glob_iters, local_epochs, optimizer, numedges, i, tau)\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/optimization/Fed-Sophia-pytorch/main.ipynb#W4sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m server\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rho' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Failed to log git patch\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error exporting current conda environment\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda package as an explicit file\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda information\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Heartbeat processing error\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Start the optimization process\n",
    "study.optimize(main, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters and their corresponding loss\n",
    "best_params = study.best_params\n",
    "best_loss = study.best_value\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sophia_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Sophia\",\n",
    "    \"model\": \"CNN\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 0.01,\n",
    "    \"L\": 1e-5,\n",
    "    \"rho\": 20,\n",
    "    \"num_glob_iters\": 300,\n",
    "    \"local_epochs\": 20,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "DONE_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"DONE\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 500,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "Newton_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Newton\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 500,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"Newton\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/ahmed-khaled-saleh/sophia-supplement/6d2f7ecd10ce497ea05a0300260c2c18\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Running time:------------ 0\n",
      "Number of edges / total edges: 30  /  32\n",
      "-------------Round number:  0  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05644859813084112\n",
      "Average Global Trainning Loss    :  2.305579572916031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error exporting current conda environment\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda package as an explicit file\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Round number:  1  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05644859813084112\n",
      "Average Global Trainning Loss    :  2.3055794313549995\n",
      "-------------Round number:  2  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056473520249221185\n",
      "Average Global Trainning Loss    :  2.3055792674422264\n",
      "-------------Round number:  3  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056473520249221185\n",
      "Average Global Trainning Loss    :  2.3055791184306145\n",
      "-------------Round number:  4  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056473520249221185\n",
      "Average Global Trainning Loss    :  2.305579036474228\n",
      "-------------Round number:  5  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056473520249221185\n",
      "Average Global Trainning Loss    :  2.3055788800120354\n",
      "-------------Round number:  6  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.305578723549843\n",
      "-------------Round number:  7  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.3055783063173294\n",
      "-------------Round number:  8  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.305578149855137\n",
      "-------------Round number:  9  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.3055780306458473\n",
      "-------------Round number:  10  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305577874183655\n",
      "-------------Round number:  11  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305577762424946\n",
      "-------------Round number:  12  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055776432156563\n",
      "-------------Round number:  13  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305577479302883\n",
      "-------------Round number:  14  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.30557731539011\n",
      "-------------Round number:  15  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305577203631401\n",
      "-------------Round number:  16  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055768236517906\n",
      "-------------Round number:  17  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305576652288437\n",
      "-------------Round number:  18  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305576555430889\n",
      "-------------Round number:  19  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305576391518116\n",
      "-------------Round number:  20  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055763095617294\n",
      "-------------Round number:  21  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305576130747795\n",
      "-------------Round number:  22  -------------\n",
      "Average Global Accuracy          :  0.057588885569166415\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055760115385056\n",
      "-------------Round number:  23  -------------\n",
      "Average Global Accuracy          :  0.057588885569166415\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055758625268936\n",
      "-------------Round number:  24  -------------\n",
      "Average Global Accuracy          :  0.057588885569166415\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305575706064701\n",
      "-------------Round number:  25  -------------\n",
      "Average Global Accuracy          :  0.057588885569166415\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305575340986252\n",
      "-------------Round number:  26  -------------\n",
      "Average Global Accuracy          :  0.057588885569166415\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055752217769623\n",
      "-------------Round number:  27  -------------\n",
      "Average Global Accuracy          :  0.057588885569166415\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305575095117092\n",
      "-------------Round number:  28  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055748715996742\n",
      "-------------Round number:  29  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055747747421265\n",
      "-------------Round number:  30  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305574730038643\n",
      "-------------Round number:  31  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055745661258698\n",
      "-------------Round number:  32  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305574409663677\n",
      "-------------Round number:  33  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.3055743128061295\n",
      "-------------Round number:  34  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.305574156343937\n",
      "-------------Round number:  35  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.30557382106781\n",
      "-------------Round number:  36  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055736422538757\n",
      "-------------Round number:  37  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305573619902134\n",
      "-------------Round number:  38  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305573485791683\n",
      "-------------Round number:  39  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305573232471943\n",
      "-------------Round number:  40  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305573210120201\n",
      "-------------Round number:  41  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055729642510414\n",
      "-------------Round number:  42  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055729642510414\n",
      "-------------Round number:  43  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055728003382683\n",
      "-------------Round number:  44  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055724278092384\n",
      "-------------Round number:  45  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305572271347046\n",
      "-------------Round number:  46  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305572137236595\n",
      "-------------Round number:  47  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305572025477886\n",
      "-------------Round number:  48  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305571898818016\n",
      "-------------Round number:  49  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305571734905243\n",
      "-------------Round number:  50  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055716529488564\n",
      "-------------Round number:  51  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055714666843414\n",
      "-------------Round number:  52  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055713325738907\n",
      "-------------Round number:  53  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305570997297764\n",
      "-------------Round number:  54  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.305570885539055\n",
      "-------------Round number:  55  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055707588791847\n",
      "-------------Round number:  56  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055706694722176\n",
      "-------------Round number:  57  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305570513010025\n",
      "-------------Round number:  58  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055703788995743\n",
      "-------------Round number:  59  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305570252239704\n",
      "-------------Round number:  60  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.3055701330304146\n",
      "-------------Round number:  61  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305570036172867\n",
      "-------------Round number:  62  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305569641292095\n",
      "-------------Round number:  63  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.3055695220828056\n",
      "-------------Round number:  64  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305569313466549\n",
      "-------------Round number:  65  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305569238960743\n",
      "-------------Round number:  66  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055690824985504\n",
      "-------------Round number:  67  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055690079927444\n",
      "-------------Round number:  68  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055688738822937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Heartbeat processing error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Round number:  69  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.305568717420101\n",
      "-------------Round number:  70  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055685609579086\n",
      "-------------Round number:  71  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055684939026833\n",
      "-------------Round number:  72  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305568091571331\n",
      "-------------Round number:  73  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055679947137833\n",
      "-------------Round number:  74  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305567853152752\n",
      "-------------Round number:  75  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055678233504295\n",
      "-------------Round number:  76  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305567644536495\n",
      "-------------Round number:  77  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305567517876625\n",
      "-------------Round number:  78  -------------\n",
      "Average Global Accuracy          :  0.057812966835972514\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055674210190773\n",
      "-------------Round number:  79  -------------\n",
      "Average Global Accuracy          :  0.05788766059157455\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055673390626907\n",
      "-------------Round number:  80  -------------\n",
      "Average Global Accuracy          :  0.05788766059157455\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305567219853401\n",
      "-------------Round number:  81  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055668026208878\n",
      "-------------Round number:  82  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055666983127594\n",
      "-------------Round number:  83  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055665865540504\n",
      "-------------Round number:  84  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055664747953415\n",
      "-------------Round number:  85  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055663779377937\n",
      "-------------Round number:  86  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055661991238594\n",
      "-------------Round number:  87  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055661469697952\n",
      "-------------Round number:  88  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055659905076027\n",
      "-------------Round number:  89  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055658861994743\n",
      "-------------Round number:  90  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055654615163803\n",
      "-------------Round number:  91  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305565394461155\n",
      "-------------Round number:  92  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.305565267801285\n",
      "-------------Round number:  93  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055651411414146\n",
      "-------------Round number:  94  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055650740861893\n",
      "-------------Round number:  95  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.305564932525158\n",
      "-------------Round number:  96  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055647760629654\n",
      "-------------Round number:  97  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305564656853676\n",
      "-------------Round number:  98  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.3055645152926445\n",
      "-------------Round number:  99  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305564410984516\n",
      "-------------Round number:  100  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.3055641278624535\n",
      "-------------Round number:  101  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.3055639415979385\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"lhdQnruUATiAZPyU7Qp2zFiVX\",\n",
    "        project_name=\"sophia-supplement\",\n",
    "        workspace=\"ahmed-khaled-saleh\",\n",
    "    )\n",
    "main(experiment, **sophia_params)\n",
    "experiment.end()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"lhdQnruUATiAZPyU7Qp2zFiVX\",\n",
    "        project_name=\"sophia-supplement\",\n",
    "        workspace=\"ahmed-khaled-saleh\",\n",
    ")\n",
    "\n",
    "main(experiment, **DONE_params)\n",
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
