{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from comet_ml import Experiment\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "from algorithms.server.server import Server\n",
    "from algorithms.trainmodel.models import *\n",
    "from utils.plot_utils import *\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def main(experiment, dataset, algorithm, model, batch_size, learning_rate, alpha, eta, L, rho, num_glob_iters,\n",
    "         local_epochs, optimizer, numedges, times, commet, gpu):\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "\n",
    "    for i in range(times):\n",
    "        print(\"---------------Running time:------------\",i)\n",
    "\n",
    "        # Generate model\n",
    "        if(model == \"mclr\"):\n",
    "            if(dataset == \"human_activity\"):\n",
    "                model = Mclr_Logistic(561,6).to(device), model\n",
    "            else:\n",
    "                model = Mclr_Logistic().to(device), model\n",
    "\n",
    "        if(model == \"linear_regression\"):\n",
    "            model = Linear_Regression(40,1).to(device), model\n",
    "\n",
    "        if model == \"logistic_regression\":\n",
    "            model = Logistic_Regression(300).to(device), model\n",
    "        \n",
    "        if model == \"MLP\" and dataset == \"a9a\":\n",
    "            model = DNN( input_dim = 123, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"human_activity\":\n",
    "            model = DNN( input_dim = 561, mid_dim = 561, output_dim = 6).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"w8a\":\n",
    "            model = DNN( input_dim = 300, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"Mnist\":\n",
    "            model = DNN().to(device), model\n",
    "        if model == 'CNN':\n",
    "            model = Net().to(device), model\n",
    "        # select algorithm\n",
    "        if(commet):\n",
    "            experiment.set_name(dataset + \"_\" + algorithm + \"_\" + model[1] + \"_\" + str(batch_size) + \"b_\" + str(learning_rate) + \"lr_\" + str(alpha) + \"al_\" + str(eta) + \"eta_\" + str(L) + \"L_\" + str(rho) + \"p_\" +  str(num_glob_iters) + \"ge_\"+ str(local_epochs) + \"le_\"+ str(numedges) +\"u\")\n",
    "        server = Server(experiment, device, dataset, algorithm, model, batch_size, learning_rate, alpha, eta,  L, num_glob_iters, local_epochs, optimizer, numedges, i)\n",
    "        \n",
    "        server.train()\n",
    "        server.test()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sophia_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Sophia\",\n",
    "    \"model\": \"CNN\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 0.01,\n",
    "    \"L\": 1e-5,\n",
    "    \"rho\": 20,\n",
    "    \"num_glob_iters\": 300,\n",
    "    \"local_epochs\": 20,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "DONE_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"DONE\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 500,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "Newton_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Newton\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 500,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"Newton\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/ahmed-khaled-saleh/sophia-supplement/6d2f7ecd10ce497ea05a0300260c2c18\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Running time:------------ 0\n",
      "Number of edges / total edges: 30  /  32\n",
      "-------------Round number:  0  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05644859813084112\n",
      "Average Global Trainning Loss    :  2.305579572916031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error exporting current conda environment\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda package as an explicit file\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Round number:  1  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05644859813084112\n",
      "Average Global Trainning Loss    :  2.3055794313549995\n",
      "-------------Round number:  2  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056473520249221185\n",
      "Average Global Trainning Loss    :  2.3055792674422264\n",
      "-------------Round number:  3  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056473520249221185\n",
      "Average Global Trainning Loss    :  2.3055791184306145\n",
      "-------------Round number:  4  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056473520249221185\n",
      "Average Global Trainning Loss    :  2.305579036474228\n",
      "-------------Round number:  5  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056473520249221185\n",
      "Average Global Trainning Loss    :  2.3055788800120354\n",
      "-------------Round number:  6  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.305578723549843\n",
      "-------------Round number:  7  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.3055783063173294\n",
      "-------------Round number:  8  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.305578149855137\n",
      "-------------Round number:  9  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.3055780306458473\n",
      "-------------Round number:  10  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305577874183655\n",
      "-------------Round number:  11  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305577762424946\n",
      "-------------Round number:  12  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055776432156563\n",
      "-------------Round number:  13  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305577479302883\n",
      "-------------Round number:  14  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.30557731539011\n",
      "-------------Round number:  15  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305577203631401\n",
      "-------------Round number:  16  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055768236517906\n",
      "-------------Round number:  17  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305576652288437\n",
      "-------------Round number:  18  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305576555430889\n",
      "-------------Round number:  19  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305576391518116\n",
      "-------------Round number:  20  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055763095617294\n",
      "-------------Round number:  21  -------------\n",
      "Average Global Accuracy          :  0.05751419181356439\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305576130747795\n",
      "-------------Round number:  22  -------------\n",
      "Average Global Accuracy          :  0.057588885569166415\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055760115385056\n",
      "-------------Round number:  23  -------------\n",
      "Average Global Accuracy          :  0.057588885569166415\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055758625268936\n",
      "-------------Round number:  24  -------------\n",
      "Average Global Accuracy          :  0.057588885569166415\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305575706064701\n",
      "-------------Round number:  25  -------------\n",
      "Average Global Accuracy          :  0.057588885569166415\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305575340986252\n",
      "-------------Round number:  26  -------------\n",
      "Average Global Accuracy          :  0.057588885569166415\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055752217769623\n",
      "-------------Round number:  27  -------------\n",
      "Average Global Accuracy          :  0.057588885569166415\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305575095117092\n",
      "-------------Round number:  28  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055748715996742\n",
      "-------------Round number:  29  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055747747421265\n",
      "-------------Round number:  30  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305574730038643\n",
      "-------------Round number:  31  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055745661258698\n",
      "-------------Round number:  32  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305574409663677\n",
      "-------------Round number:  33  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.3055743128061295\n",
      "-------------Round number:  34  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.305574156343937\n",
      "-------------Round number:  35  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.30557382106781\n",
      "-------------Round number:  36  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055736422538757\n",
      "-------------Round number:  37  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305573619902134\n",
      "-------------Round number:  38  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305573485791683\n",
      "-------------Round number:  39  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305573232471943\n",
      "-------------Round number:  40  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305573210120201\n",
      "-------------Round number:  41  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055729642510414\n",
      "-------------Round number:  42  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055729642510414\n",
      "-------------Round number:  43  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055728003382683\n",
      "-------------Round number:  44  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055724278092384\n",
      "-------------Round number:  45  -------------\n",
      "Average Global Accuracy          :  0.05766357932476845\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305572271347046\n",
      "-------------Round number:  46  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305572137236595\n",
      "-------------Round number:  47  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305572025477886\n",
      "-------------Round number:  48  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305571898818016\n",
      "-------------Round number:  49  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305571734905243\n",
      "-------------Round number:  50  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055716529488564\n",
      "-------------Round number:  51  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055714666843414\n",
      "-------------Round number:  52  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055713325738907\n",
      "-------------Round number:  53  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.305570997297764\n",
      "-------------Round number:  54  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056498442367601244\n",
      "Average Global Trainning Loss    :  2.305570885539055\n",
      "-------------Round number:  55  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055707588791847\n",
      "-------------Round number:  56  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055706694722176\n",
      "-------------Round number:  57  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305570513010025\n",
      "-------------Round number:  58  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055703788995743\n",
      "-------------Round number:  59  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305570252239704\n",
      "-------------Round number:  60  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.3055701330304146\n",
      "-------------Round number:  61  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305570036172867\n",
      "-------------Round number:  62  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305569641292095\n",
      "-------------Round number:  63  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.3055695220828056\n",
      "-------------Round number:  64  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305569313466549\n",
      "-------------Round number:  65  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305569238960743\n",
      "-------------Round number:  66  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055690824985504\n",
      "-------------Round number:  67  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055690079927444\n",
      "-------------Round number:  68  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055688738822937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Heartbeat processing error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Round number:  69  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.305568717420101\n",
      "-------------Round number:  70  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055685609579086\n",
      "-------------Round number:  71  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055684939026833\n",
      "-------------Round number:  72  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305568091571331\n",
      "-------------Round number:  73  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055679947137833\n",
      "-------------Round number:  74  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305567853152752\n",
      "-------------Round number:  75  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055678233504295\n",
      "-------------Round number:  76  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305567644536495\n",
      "-------------Round number:  77  -------------\n",
      "Average Global Accuracy          :  0.05773827308037048\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305567517876625\n",
      "-------------Round number:  78  -------------\n",
      "Average Global Accuracy          :  0.057812966835972514\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055674210190773\n",
      "-------------Round number:  79  -------------\n",
      "Average Global Accuracy          :  0.05788766059157455\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055673390626907\n",
      "-------------Round number:  80  -------------\n",
      "Average Global Accuracy          :  0.05788766059157455\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305567219853401\n",
      "-------------Round number:  81  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05652336448598131\n",
      "Average Global Trainning Loss    :  2.3055668026208878\n",
      "-------------Round number:  82  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055666983127594\n",
      "-------------Round number:  83  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055665865540504\n",
      "-------------Round number:  84  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055664747953415\n",
      "-------------Round number:  85  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055663779377937\n",
      "-------------Round number:  86  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055661991238594\n",
      "-------------Round number:  87  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055661469697952\n",
      "-------------Round number:  88  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055659905076027\n",
      "-------------Round number:  89  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055658861994743\n",
      "-------------Round number:  90  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.3055654615163803\n",
      "-------------Round number:  91  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.05654828660436137\n",
      "Average Global Trainning Loss    :  2.305565394461155\n",
      "-------------Round number:  92  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.305565267801285\n",
      "-------------Round number:  93  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055651411414146\n",
      "-------------Round number:  94  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055650740861893\n",
      "-------------Round number:  95  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.305564932525158\n",
      "-------------Round number:  96  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056573208722741436\n",
      "Average Global Trainning Loss    :  2.3055647760629654\n",
      "-------------Round number:  97  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305564656853676\n",
      "-------------Round number:  98  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.3055645152926445\n",
      "-------------Round number:  99  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.305564410984516\n",
      "-------------Round number:  100  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.3055641278624535\n",
      "-------------Round number:  101  -------------\n",
      "Average Global Accuracy          :  0.05796235434717657\n",
      "Average Global Trainning Accuracy:  0.056598130841121495\n",
      "Average Global Trainning Loss    :  2.3055639415979385\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"lhdQnruUATiAZPyU7Qp2zFiVX\",\n",
    "        project_name=\"sophia-supplement\",\n",
    "        workspace=\"ahmed-khaled-saleh\",\n",
    "    )\n",
    "main(experiment, **sophia_params)\n",
    "experiment.end()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"lhdQnruUATiAZPyU7Qp2zFiVX\",\n",
    "        project_name=\"sophia-supplement\",\n",
    "        workspace=\"ahmed-khaled-saleh\",\n",
    ")\n",
    "\n",
    "main(experiment, **DONE_params)\n",
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
